{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa2fd7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 目的：以 TCN 預測 y_dir_6h，3 折走勢前移驗證，校準機率，記錄 MLflow\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, precision_recall_fscore_support\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tcn import TCN\n",
    "\n",
    "# ========== 0) 參數 ==========\n",
    "FEAT_PATH = Path(\"./data/feat_6h.parquet\")\n",
    "LABEL_PATH = Path(\"./data/label_6h.parquet\")\n",
    "ART_DIR = Path(\"./artifacts_tcn\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOOKBACK = 16\n",
    "TEST_DAYS = 90\n",
    "TRAIN_MIN_DAYS = 120  # 若資料不足，至少用這麼多天作訓練\n",
    "FREQ_PER_DAY = 4      # 6h 一天四筆\n",
    "TRACKING_URI = \"mlruns\"\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(\"tcn_6h\")\n",
    "\n",
    "# ========== 1) 載入與合併 ==========\n",
    "feat = pd.read_parquet(FEAT_PATH)\n",
    "label = pd.read_parquet(LABEL_PATH)\n",
    "\n",
    "df = feat.merge(label, on=\"time\", how=\"inner\").sort_values(\"time\").reset_index(drop=True)\n",
    "times = pd.to_datetime(df[\"time\"], utc=True)\n",
    "\n",
    "# 特徵與標籤\n",
    "y = df[\"y_dir_6h\"].astype(int).values\n",
    "X = df.drop(columns=[\"time\", \"y_dir_6h\", \"y_tail_6h\"]).values\n",
    "\n",
    "# ========== 2) 切分訓練/測試（最後 90 天為測試） ==========\n",
    "test_start_ts = times.max() - pd.Timedelta(days=TEST_DAYS)\n",
    "train_mask = times < test_start_ts\n",
    "test_mask = times >= test_start_ts\n",
    "\n",
    "X_train_raw, y_train_raw, t_train = X[train_mask], y[train_mask], times[train_mask]\n",
    "X_test_raw, y_test_raw, t_test = X[test_mask], y[test_mask], times[test_mask]\n",
    "\n",
    "# 特徵縮放：只在訓練集 fit\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "joblib.dump(scaler, ART_DIR / \"scaler.pkl\")\n",
    "\n",
    "# ========== 3) 建立序列資料 ==========\n",
    "def make_sequences(X2d: np.ndarray, y1d: np.ndarray, lookback: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    Xseq = []\n",
    "    yseq = []\n",
    "    for i in range(lookback, len(X2d)):\n",
    "        Xseq.append(X2d[i - lookback:i, :])\n",
    "        yseq.append(y1d[i])\n",
    "    return np.asarray(Xseq, dtype=np.float32), np.asarray(yseq, dtype=np.float32)\n",
    "\n",
    "X_train_seq, y_train_seq = make_sequences(X_train, y_train_raw, LOOKBACK)\n",
    "X_test_seq, y_test_seq = make_sequences(X_test, y_test_raw, LOOKBACK)\n",
    "\n",
    "# 為 walk-forward 分割保留時間戳對齊\n",
    "t_train_seq = t_train[LOOKBACK:]\n",
    "\n",
    "# ========== 4) Walk-Forward 折切分 ==========\n",
    "def gen_walk_forward_splits(times_seq: pd.Series, train_days: int = 270, val_days: int = 90) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    splits = []\n",
    "    start_date = times_seq.min()\n",
    "    end_date = times_seq.max()\n",
    "    cursor = start_date\n",
    "    while True:\n",
    "        train_end = cursor + pd.Timedelta(days=train_days)\n",
    "        val_end = train_end + pd.Timedelta(days=val_days)\n",
    "        if val_end > end_date:\n",
    "            break\n",
    "        train_idx = (times_seq >= cursor) & (times_seq < train_end)\n",
    "        val_idx = (times_seq >= train_end) & (times_seq < val_end)\n",
    "        if train_idx.sum() > LOOKBACK * 2 and val_idx.sum() > LOOKBACK:\n",
    "            splits.append((np.where(train_idx)[0], np.where(val_idx)[0]))\n",
    "        cursor = cursor + pd.Timedelta(days=val_days)\n",
    "    # 若無法產生任何合法切分，退化為 1 折 70/30\n",
    "    if not splits:\n",
    "        n = len(times_seq)\n",
    "        cut = int(n * 0.7)\n",
    "        splits = [(np.arange(0, cut), np.arange(cut, n))]\n",
    "    return splits\n",
    "\n",
    "cv_splits = gen_walk_forward_splits(pd.Series(t_train_seq))\n",
    "\n",
    "# ========== 5) 建模函式 ==========\n",
    "def build_tcn(input_shape, nb_filters=64, kernel_size=3, dilations=(1,2,4,8,16,32), dropout_rate=0.1) -> keras.Model:\n",
    "    inp = keras.Input(shape=input_shape)\n",
    "    x = TCN(nb_filters=nb_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilations=dilations,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation='relu',\n",
    "            padding='causal',\n",
    "            return_sequences=False)(inp)\n",
    "    out = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[keras.metrics.AUC(name=\"auc\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "param_grid = [\n",
    "    dict(nb_filters=64, kernel_size=3, dilations=(1,2,4,8,16,32), dropout_rate=0.1),\n",
    "    dict(nb_filters=64, kernel_size=5, dilations=(1,2,4,8,16), dropout_rate=0.1),\n",
    "    dict(nb_filters=96, kernel_size=3, dilations=(1,2,4,8,16,32), dropout_rate=0.2),\n",
    "]\n",
    "\n",
    "# ========== 6) 超參數搜尋（WF 3 折） ==========\n",
    "best_params = None\n",
    "best_cv_auc = -np.inf\n",
    "oof_preds = []  # 收集驗證集預測以供校準\n",
    "oof_labels = []\n",
    "\n",
    "with mlflow.start_run(run_name=\"tcn_cv\"):\n",
    "    for params in param_grid:\n",
    "        fold_aucs = []\n",
    "        for (tr_idx, va_idx) in cv_splits:\n",
    "            model = build_tcn(\n",
    "                input_shape=(LOOKBACK, X_train_seq.shape[-1]),\n",
    "                **params\n",
    "            )\n",
    "            es = keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_auc\", mode=\"max\", patience=10, restore_best_weights=True\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_seq[tr_idx], y_train_seq[tr_idx],\n",
    "                validation_data=(X_train_seq[va_idx], y_train_seq[va_idx]),\n",
    "                epochs=100,\n",
    "                batch_size=128,\n",
    "                callbacks=[es],\n",
    "                verbose=0\n",
    "            )\n",
    "            va_pred = model.predict(X_train_seq[va_idx], verbose=0).ravel()\n",
    "            auc = roc_auc_score(y_train_seq[va_idx], va_pred)\n",
    "            fold_aucs.append(auc)\n",
    "        mean_auc = float(np.mean(fold_aucs))\n",
    "        mlflow.log_param(f\"params_{params}\", str(params))\n",
    "        mlflow.log_metric(f\"cv_auc_{params}\", mean_auc)\n",
    "        if mean_auc > best_cv_auc:\n",
    "            best_cv_auc = mean_auc\n",
    "            best_params = params\n",
    "\n",
    "    # 以最佳參數重跑一次，收集全部驗證預測做校準\n",
    "    for (tr_idx, va_idx) in cv_splits:\n",
    "        model = build_tcn(input_shape=(LOOKBACK, X_train_seq.shape[-1]), **best_params)\n",
    "        es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=10, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train_seq[tr_idx], y_train_seq[tr_idx],\n",
    "            validation_data=(X_train_seq[va_idx], y_train_seq[va_idx]),\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            callbacks=[es],\n",
    "            verbose=0\n",
    "        )\n",
    "        va_pred = model.predict(X_train_seq[va_idx], verbose=0).ravel()\n",
    "        oof_preds.append(va_pred)\n",
    "        oof_labels.append(y_train_seq[va_idx])\n",
    "\n",
    "    oof_preds = np.concatenate(oof_preds)\n",
    "    oof_labels = np.concatenate(oof_labels)\n",
    "    mlflow.log_metric(\"best_cv_auc\", float(best_cv_auc))\n",
    "\n",
    "# ========== 7) 最終模型（整段訓練集） ==========\n",
    "final_model = build_tcn(input_shape=(LOOKBACK, X_train_seq.shape[-1]), **best_params)\n",
    "es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=12, restore_best_weights=True)\n",
    "# 使用訓練集中末尾 10% 作內部驗證以利早停\n",
    "cut = int(len(X_train_seq) * 0.9)\n",
    "final_model.fit(\n",
    "    X_train_seq[:cut], y_train_seq[:cut],\n",
    "    validation_data=(X_train_seq[cut:], y_train_seq[cut:]),\n",
    "    epochs=120,\n",
    "    batch_size=128,\n",
    "    callbacks=[es],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 測試集評估\n",
    "proba_test = final_model.predict(X_test_seq, verbose=0).ravel()\n",
    "auc_test = roc_auc_score(y_test_seq, proba_test)\n",
    "brier_test = brier_score_loss(y_test_seq, proba_test)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support((proba_test >= 0.5).astype(int), y_test_seq, average=\"binary\", zero_division=0)\n",
    "\n",
    "# ========== 8) 機率校準（Isotonic） ==========\n",
    "calibrator = IsotonicRegression(out_of_bounds=\"clip\").fit(oof_preds, oof_labels)\n",
    "proba_test_cal = calibrator.transform(proba_test)\n",
    "brier_test_cal = brier_score_loss(y_test_seq, proba_test_cal)\n",
    "\n",
    "# 保存\n",
    "final_model.save(ART_DIR / \"tcn_model.h5\")\n",
    "joblib.dump(calibrator, ART_DIR / \"tcn_calibrator.pkl\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"tcn_final\"):\n",
    "    mlflow.log_param(\"lookback\", LOOKBACK)\n",
    "    mlflow.log_param(\"best_params\", str(best_params))\n",
    "    mlflow.log_metric(\"test_auc_raw\", float(auc_test))\n",
    "    mlflow.log_metric(\"test_brier_raw\", float(brier_test))\n",
    "    mlflow.log_metric(\"test_brier_cal\", float(brier_test_cal))\n",
    "    mlflow.log_metric(\"precision@0.5\", float(prec))\n",
    "    mlflow.log_metric(\"recall@0.5\", float(rec))\n",
    "    mlflow.log_metric(\"f1@0.5\", float(f1))\n",
    "    mlflow.log_artifact(str(ART_DIR / \"tcn_model.h5\"))\n",
    "    mlflow.log_artifact(str(ART_DIR / \"tcn_calibrator.pkl\"))\n",
    "    mlflow.log_artifact(str(ART_DIR / \"scaler.pkl\"))\n",
    "\n",
    "print({\n",
    "    \"best_cv_auc\": float(best_cv_auc),\n",
    "    \"test_auc_raw\": float(auc_test),\n",
    "    \"test_brier_raw\": float(brier_test),\n",
    "    \"test_brier_cal\": float(brier_test_cal),\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
